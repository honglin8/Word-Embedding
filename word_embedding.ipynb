{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Implement a simple word embedding in Python (from scratch) and use it to find the most similar words to a given word. Come up with a dataset and evaluation metrics to evaluate the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK's stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'yours', 'doesn', \"hadn't\", 'is', \"that'll\", \"you're\", 'couldn', 'into', 'while', 'yourself', 've', 'your', 'so', 'how', 'before', 'wouldn', 'should', 'why', 'too', 'as', 'with', 'she', 'ours', 'hers', 'you', 'in', 'each', 'because', 't', 'can', \"hasn't\", 'shouldn', 'himself', \"shan't\", \"aren't\", \"you'll\", 'it', 'am', 'and', 'won', 'both', 'some', 'had', 'own', 'having', 'have', 'a', 'my', 'now', 'hasn', \"she's\", 'been', 'or', 'our', 'over', 'again', 'few', \"should've\", 'further', 'needn', 'more', \"wasn't\", \"weren't\", 'to', 'i', 'myself', 'y', 'weren', 'for', 'down', 'nor', 'its', 'which', 'out', 'be', 'off', \"shouldn't\", 'other', 'what', 'were', 'aren', 'until', 'once', 'they', 'yourselves', 'her', 'an', 'if', 'under', 'from', \"it's\", 'mustn', 'any', 'the', 'haven', 'who', 'only', \"doesn't\", 'd', 'll', \"didn't\", 'here', 'shan', 'ain', 'at', \"needn't\", 'between', 'ourselves', 'we', 'itself', 'no', 'will', 'against', 'below', 'where', 'do', 'did', 'he', \"won't\", 'are', 'up', 'that', 'being', 'themselves', 'not', 'doing', 'during', 'was', 'but', 'about', 'after', 'there', 'm', 'these', 'mightn', 'when', 'theirs', 'isn', \"wouldn't\", 'does', 're', \"you'd\", \"couldn't\", \"mightn't\", 'wasn', 'herself', 'through', 'didn', 'them', 'just', \"haven't\", 'than', 's', 'then', 'o', \"isn't\", 'very', 'by', 'this', \"mustn't\", \"you've\", 'their', 'such', 'above', 'hadn', 'those', 'all', 'has', 'him', 'ma', 'most', 'whom', 'same', 'me', 'his', \"don't\", 'of', 'don'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\") # Uncomment this line to download the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Brown Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"brown\") # Uncomment this line to download the brown corpus\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the categories of the brown corpus \n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4249\n",
      "Sentence 0: ['Thirty-three']\n",
      "Sentence 1: ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.']\n",
      "Sentence 2: ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.']\n",
      "Sentence 3: ['His', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.']\n",
      "Sentence 4: ['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.']\n"
     ]
    }
   ],
   "source": [
    "# Get the sentences from the \"fiction\" category\n",
    "sentences = brown.sents(categories=\"fiction\")\n",
    "\n",
    "# Number of sentences\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i}: {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'did', 'not', 'go', 'back', 'to', 'school']\n",
      "Filtered sentence 2: ['his', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'university', 'hospital', 'mckinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', 'for', 'the', 'rest', 'do', 'pretty', 'much', 'as', 'he', 'chose', 'provided', 'of', 'course', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating']\n",
      "Filtered sentence 3: ['his', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-alphanumeric words and convert to lowercase\n",
    "filtered_sentences = []\n",
    "\n",
    "for sent in sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum()]\n",
    "    filtered_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'go', 'back', 'school']\n",
      "Filtered sentence 2: ['parents', 'talked', 'seriously', 'lengthily', 'doctor', 'specialist', 'university', 'hospital', 'mckinley', 'entitled', 'discount', 'members', 'family', 'decided', 'would', 'best', 'take', 'remainder', 'term', 'spend', 'lot', 'time', 'bed', 'rest', 'pretty', 'much', 'chose', 'provided', 'course', 'chose', 'nothing', 'exciting', 'debilitating']\n",
      "Filtered sentence 3: ['teacher', 'school', 'principal', 'conferred', 'everyone', 'agreed', 'kept', 'certain', 'amount', 'work', 'home', 'little', 'danger', 'losing', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'decision', 'indifference', 'enter', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords and non-alphanumeric words and convert to lowercase\n",
    "filtered_stopwords_sentences = []\n",
    "\n",
    "for sent in filtered_sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum() and word.lower() not in stopwords]\n",
    "    filtered_stopwords_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_stopwords_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Skip-Gram, the goal is to predict the context words $w_c$ given a target word $w_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-Gram with Negative Sampling (SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling simplifies this process by approximating the softmax function, \n",
    "focusing only on a few context words (positive samples) and a few randomly selected words from the vocabulary (negative samples) instead of the entire vocabulary.\n",
    "\n",
    "Loss Function: \\\n",
    "The loss function for Skip-Gram with Negative Sampling (SGNS) is typically a binary cross-entropy loss. For each target word $w_t$ and its context words $w_c$, the loss function tries to:\n",
    "- Maximize the probability that $w_c$ appears in the context of $w_t$\n",
    "- Minimize the probability that randomly selected (negative) words appear in the context\n",
    "\n",
    "The loss function is\n",
    "$$ L = - \\left[ \\sum_{w_c \\in \\text{context}} \\log \\sigma \\left( \\langle v_{w_c}, v_{w_t} \\rangle \\right) \n",
    "             +  \\sum_{w_n \\in \\text{negatives}} \\log \\sigma \\left( \\langle -v_{w_n}, v_{w_t} \\rangle \\right) \\right] $$\n",
    "\n",
    "where \n",
    "- $v_{w_t}$ is the embedding vector of target word $w_t$\n",
    "- $v_{w_c}$ is the embedding vector of context word $w_c$\n",
    "- $v_{w_n}$ is the embedding vector of negative word $w_n$ \n",
    "- $\\sigma(x)$ is the sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build vocabulary index\n",
    "def build_vocab_idx(filtered_sentences):\n",
    "    vocab_count = defaultdict(int)\n",
    "    for sentence in filtered_sentences:\n",
    "        for word in sentence:\n",
    "            vocab_count[word] += 1\n",
    "    return {word: idx for idx, (word, _) in enumerate(vocab_count.items())}, vocab_count\n",
    "\n",
    "embedding_dim = 100 \n",
    "vocab_idx, vocab_count = build_vocab_idx(filtered_sentences)\n",
    "vocab_size = len(vocab_idx)\n",
    "\n",
    "W_in = jax.random.normal(jax.random.PRNGKey(0), (vocab_size, embedding_dim)) * 0.01\n",
    "W_out = jax.random.normal(jax.random.PRNGKey(1), (vocab_size, embedding_dim)) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: W_in, W_out, target word index in vocab_idx, contexts & weights output of get_context_words(), \n",
    "#   negative_samples output of get_negative_samples()\n",
    "# output: loss value\n",
    "def loss_fn(W_in, W_out, target, contexts, negative_samples, weights):\n",
    "    # Get word embeddings\n",
    "    target_embedding = W_in[target]\n",
    "\n",
    "    pos_loss = 0\n",
    "    neg_loss = 0\n",
    "    \n",
    "    # Positive score\n",
    "    for idx, i in enumerate(contexts):\n",
    "        weight = weights[idx]\n",
    "        context_embedding = W_out[i]\n",
    "        positive_score = jnp.dot(context_embedding, target_embedding)\n",
    "        pos_loss += jnp.log(jax.nn.sigmoid(positive_score)) * weight\n",
    "\n",
    "    # Negative scores and loss\n",
    "    for j in negative_samples:\n",
    "        negative_embedding = W_out[j]\n",
    "        negative_scores = jnp.dot(-negative_embedding, target_embedding)\n",
    "        neg_loss += jnp.log(jax.nn.sigmoid(-negative_scores))\n",
    "\n",
    "    # Total loss is the sum of positive loss and negative loss\n",
    "    return -(pos_loss + neg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Context Words & Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: target word index in sentence, selected sentence, number of context words\n",
    "# output: list of context words index linked to vocab_idx, weights\n",
    "def get_context_words(target_idx, sentence, num_context_words):\n",
    "    window_size = num_context_words // 2\n",
    "    \n",
    "    start = max(0, target_idx - window_size)\n",
    "    end = min(len(sentence), target_idx + window_size + 1)\n",
    "\n",
    "    contexts = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        if i != target_idx:\n",
    "            contexts.append(vocab_idx[sentence[i]])\n",
    "            weights.append(1/abs(i-target_idx))\n",
    "    \n",
    "    return contexts, weights\n",
    "\n",
    "\n",
    "# input: target word index in vocab_idx, number of unique words, number of negative samples, contexts output of get_context_words()\n",
    "# output: list of negative words index linked to vocab_idx\n",
    "def get_negative_samples(target, vocab_size, num_negative_samples, contexts):\n",
    "    negative_samples = set()\n",
    "    \n",
    "    while len(negative_samples) < num_negative_samples:\n",
    "        # Sample a random word from the vocabulary\n",
    "        neg_sample = np.random.randint(0, vocab_size)\n",
    "        # Ensure it's not the target word or context word\n",
    "        if neg_sample != target and neg_sample not in contexts:\n",
    "            negative_samples.add(neg_sample)\n",
    "    \n",
    "    return list(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.00905581,  0.01658525,  0.0097903 , ..., -0.01400041,\n",
       "        -0.00166153,  0.00707039],\n",
       "       [ 0.00160605,  0.00497563, -0.00062179, ...,  0.00591333,\n",
       "        -0.01230938,  0.00494421],\n",
       "       [-0.00273051,  0.00624131,  0.00508934, ...,  0.0074587 ,\n",
       "         0.00509201, -0.01680029],\n",
       "       ...,\n",
       "       [-0.02000904, -0.0047469 , -0.00137689, ...,  0.00923165,\n",
       "        -0.01735099,  0.00018416],\n",
       "       [ 0.01096799, -0.01939535, -0.01600649, ..., -0.0066917 ,\n",
       "        -0.00204523,  0.01560743],\n",
       "       [-0.00563035, -0.01122078,  0.00938086, ...,  0.00206636,\n",
       "         0.01129831, -0.00139228]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "num_negative_samples = 5\n",
    "num_context_words = 4\n",
    "learning_rate = 0.01\n",
    "num_epoch = 1\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for sentence in filtered_stopwords_sentences:\n",
    "        for target_idx, word in enumerate(sentence):\n",
    "            target = vocab_idx[word]\n",
    "            \n",
    "            contexts, weights = get_context_words(target_idx, sentence, num_context_words)\n",
    "            negative_samples = get_negative_samples(target, vocab_size, num_negative_samples, contexts)\n",
    "\n",
    "            loss = loss_fn(W_in, W_out, target, contexts, negative_samples, weights)\n",
    "            gradients = jax.grad(loss_fn)(W_in, W_out, target, contexts, negative_samples, weights)\n",
    "\n",
    "            # Update embeddings\n",
    "            W_in -= learning_rate * gradients[0]\n",
    "            W_out -= learning_rate * gradients[1]\n",
    "\n",
    "W_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('judging', 0.0039343126),\n",
       " ('knob', 0.0039162906),\n",
       " ('needled', 0.0038007498),\n",
       " ('ring', 0.0037374797),\n",
       " ('stood', 0.0037113489)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_context_words(target_word, vocab_idx, W_in, W_out, top_n=5):\n",
    "    target_vec = W_in[vocab_idx[target_word]]\n",
    "    scores = np.dot(W_out, target_vec)  # Get scores for all words\n",
    "    top_indices = np.argsort(scores)[-top_n:][::-1]  # Get indices of top_n scores\n",
    "    predicted_words = [(list(vocab_idx.keys())[i], scores[i]) for i in top_indices]\n",
    "    \n",
    "    return predicted_words\n",
    "\n",
    "# Example usage\n",
    "predicted_contexts = predict_context_words('students', vocab_idx, W_in, W_out)\n",
    "predicted_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Skip-Gram with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Softmax is an alternative to Negative Sampling, where the softmax function is approximated using a binary tree. The loss function is structured similarly to SGNS but involves traversing the binary tree structure and computing a conditional probability at each node.\n",
    "\n",
    "In this case, the loss function minimizes the difference between the predicted probability and the actual label for each word pair, traversing the tree for each prediction. The key idea is to reduce the computational complexity of softmax when dealing with a large vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CBOW, the objective is to predict the target word ​$w_t$ given the context words $w_c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  CBOW with Negative Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes the average of the context word embeddings and tries to predict the target word. The loss function for CBOW with Negative Sampling is also binary cross-entropy but formulated slightly differently due to the averaging of context word embeddings.\n",
    "\n",
    "The loss function is\n",
    "$$ L = - \\left[ \\log \\sigma \\left( \\langle v_{w_t}, v_C \\rangle \\right) \n",
    "             +  \\sum_{w_n \\in \\text{negatives}} \\log \\sigma \\left( \\langle -v_{w_n}, v_C \\rangle \\right) \\right] $$  \n",
    "where \n",
    "$$ v_C = \\frac{1}{|C|} \\sum_{w_c \\in C} v_{w_c} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Skip-Gram, CBOW can also use hierarchical softmax as an alternative to Negative Sampling, traversing a binary tree to approximate the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all words, remove all punctuations and stop words\n",
    "def preprocess_sentence(sentence):\n",
    "    return [word.lower() for word in sentence if word.lower() not in stopwords and word not in punctuation]\n",
    "\n",
    "corpus_preprocessed = [preprocess_sentence(sentence) for sentence in corpus]\n",
    "corpus_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Window size for Co-occurrence matrix - 2 words before, 2 words after\n",
    "window_size = 2\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "def build_cooccurrence_matrix(corpus_preprocessed, vocab_idx, window_size):\n",
    "    cooccurrence_matrix = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for sentence in corpus_preprocessed:\n",
    "        sentence_length = len(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            word_idx = vocab_idx[word]\n",
    "            # Context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(sentence_length, i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                # Skip target word\n",
    "                if i != j:\n",
    "                    context_word = sentence[j]\n",
    "                    context_word_idx = vocab_idx[context_word]\n",
    "                    # Increment the co-occurrence count with inverse distance weighting\n",
    "                    cooccurrence_matrix[word_idx][context_word_idx] += 1.0 / abs(i - j)\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(corpus_preprocessed, vocab_idx, window_size)\n",
    "cooccurrence_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "embedding_dim = 50\n",
    "\n",
    "# Initialize word vectors and bias\n",
    "key = jax.random.PRNGKey(0)\n",
    "v = jax.random.normal(key, (vocab_size, embedding_dim))\n",
    "key, subkey = jax.random.split(key)\n",
    "v_tilde = jax.random.normal(subkey, (vocab_size, embedding_dim))\n",
    "bias = jnp.zeros(vocab_size)\n",
    "bias_tilde = jnp.zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight function\n",
    "def f(x):\n",
    "    return jax.minimum(1.0, (x/x_max)**alpha)\n",
    "\n",
    "# loss function\n",
    "def gLove_loss(v, v_tilde, bias, bias_tilde, cooccurrence_matrix):\n",
    "    loss = 0.0\n",
    "    for word in cooccurrence_matrix:\n",
    "        for context_word in cooccurrence_matrix[word]:\n",
    "            X_ij = cooccurrence_matrix[word][context_word]\n",
    "            weight = f(X_ij)\n",
    "            diff = jnp.dot(v[word], v_tilde[context_word]) + bias[word] + bias_tilde[context_word] - jnp.log(X_ij)\n",
    "            loss += weight * diff**2\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
