{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Implement a simple word embedding in Python (from scratch) and use it to find the most similar words to a given word. Come up with a dataset and evaluation metrics to evaluate the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK's stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'your', 'you', 'the', 'having', 'my', 'he', 'where', 'off', 'such', 'isn', 'all', 'for', 'shan', 'him', 'been', 'm', 'was', \"aren't\", 'yourself', \"wasn't\", 'mightn', 'yours', 'up', 'mustn', 'itself', \"weren't\", 'most', 'same', 'wouldn', 'down', 'll', \"wouldn't\", 're', 'themselves', 'does', 'are', \"hadn't\", 'then', 'when', 'or', \"isn't\", 'each', 'under', 'did', 'be', 'those', 'because', 'can', 'theirs', 'we', 'against', 'through', 'were', 'whom', 't', 'once', 'am', 'me', 'should', \"doesn't\", 'ma', 'other', 'just', \"you'll\", 'aren', 'after', 'her', 'herself', \"it's\", 've', 'myself', 'in', 'at', 'these', 'have', 'before', 'who', 'y', 'don', 'very', 'hers', 'they', 'but', 'by', 'here', 'no', 'hadn', \"shouldn't\", 's', 'is', 'our', 'that', 'it', 'again', 'of', \"should've\", 'how', 'doesn', 'out', 'o', 'do', 'being', \"you'd\", 'd', \"won't\", 'over', 'both', \"you've\", 'their', 'she', 'yourselves', 'nor', 'few', 'will', 'between', 'couldn', 'and', \"mustn't\", 'why', 'some', 'below', 'with', 'i', \"she's\", 'about', \"don't\", 'this', 'haven', \"that'll\", 'wasn', 'shouldn', 'a', \"shan't\", 'doing', 'to', 'them', 'his', 'has', 'didn', 'any', 'needn', 'hasn', 'on', 'into', 'so', \"hasn't\", \"haven't\", 'won', 'not', 'as', 'than', 'himself', \"you're\", 'above', 'from', 'more', 'weren', 'which', 'ain', 'ourselves', 'while', \"mightn't\", 'ours', 'during', \"needn't\", 'too', 'further', 'own', 'what', 'if', 'now', 'only', 'its', 'until', \"couldn't\", 'there', \"didn't\", 'an'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\") # Uncomment this line to download the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Brown Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"brown\") # Uncomment this line to download the brown corpus\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the categories of the brown corpus \n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4249\n",
      "Sentence 0: ['Thirty-three']\n",
      "Sentence 1: ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.']\n",
      "Sentence 2: ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.']\n",
      "Sentence 3: ['His', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.']\n",
      "Sentence 4: ['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.']\n"
     ]
    }
   ],
   "source": [
    "# Get the sentences from the \"fiction\" category\n",
    "sentences = brown.sents(categories=\"fiction\")\n",
    "\n",
    "# Number of sentences\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i}: {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'did', 'not', 'go', 'back', 'to', 'school']\n",
      "Filtered sentence 2: ['his', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'university', 'hospital', 'mckinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', 'for', 'the', 'rest', 'do', 'pretty', 'much', 'as', 'he', 'chose', 'provided', 'of', 'course', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating']\n",
      "Filtered sentence 3: ['his', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-alphanumeric words and convert to lowercase\n",
    "filtered_sentences = []\n",
    "\n",
    "for sent in sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum()]\n",
    "    filtered_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'go', 'back', 'school']\n",
      "Filtered sentence 2: ['parents', 'talked', 'seriously', 'lengthily', 'doctor', 'specialist', 'university', 'hospital', 'mckinley', 'entitled', 'discount', 'members', 'family', 'decided', 'would', 'best', 'take', 'remainder', 'term', 'spend', 'lot', 'time', 'bed', 'rest', 'pretty', 'much', 'chose', 'provided', 'course', 'chose', 'nothing', 'exciting', 'debilitating']\n",
      "Filtered sentence 3: ['teacher', 'school', 'principal', 'conferred', 'everyone', 'agreed', 'kept', 'certain', 'amount', 'work', 'home', 'little', 'danger', 'losing', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'decision', 'indifference', 'enter', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords and non-alphanumeric words and convert to lowercase\n",
    "filtered_stopwords_sentences = []\n",
    "\n",
    "for sent in filtered_sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum() and word.lower() not in stopwords]\n",
    "    filtered_stopwords_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_stopwords_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Skip-Gram, the goal is to predict the context words $w_c$ given a target word $w_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build vocabulary index\n",
    "def build_vocab(sentences):\n",
    "    word_count = defaultdict(int)\n",
    "\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            word_count[word] += 1\n",
    "\n",
    "    word_to_idx = {}\n",
    "    idx_to_word = {}\n",
    "\n",
    "    for i, word in enumerate(word_count):\n",
    "        word_to_idx[word] = i\n",
    "        idx_to_word[i] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, word_count\n",
    "\n",
    "# One-hot encode a word\n",
    "def OHE_word(word, word_to_idx):\n",
    "    x = [0] * len(word_to_idx)\n",
    "    x[word_to_idx[word]] = 1\n",
    "    return jnp.array(x)\n",
    "\n",
    "# Get the context words for a target word in a sentence\n",
    "def get_context_words(target_idx, sentence, window_size=2):\n",
    "    start = max(0, target_idx - window_size)\n",
    "    end = min(len(sentence), target_idx + window_size + 1)\n",
    "\n",
    "    context_words = []\n",
    "    for context_idx in range(start, end):\n",
    "        if context_idx != target_idx:\n",
    "            context_words.append(sentence[context_idx])\n",
    "    return context_words\n",
    "\n",
    "# Generate training data from sentences\n",
    "def generate_training_data(sentences, word_to_idx, window_size=2):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        for target_idx in range(len(sent)):\n",
    "            target_word = sent[target_idx]\n",
    "            context_words = get_context_words(target_idx, sent, window_size)\n",
    "\n",
    "            for context_word in context_words:\n",
    "                X.append(OHE_word(target_word, word_to_idx))\n",
    "                Y.append(OHE_word(context_word, word_to_idx))\n",
    "\n",
    "    X = jnp.array(X)\n",
    "    Y = jnp.array(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n",
    "\n",
    "# Forward pass of the neural network\n",
    "@jax.jit\n",
    "def net(params, # the list of paramaters\n",
    "        x,      # a single one-hot encoded word, of correct shape\n",
    "        ):\n",
    "    # Initialize the output\n",
    "    x_out = x\n",
    "\n",
    "    # Get the layers\n",
    "    N_layers = len(params)\n",
    "    for layer_idx in range(N_layers - 1): # Last layer is different\n",
    "        (A,)  = params[layer_idx]\n",
    "        x_out = A@x_out\n",
    "\n",
    "    # Last layer uses softmax\n",
    "    (A,) = params[-1]\n",
    "    x_out = softmax(A@x_out)\n",
    "\n",
    "    return x_out\n",
    "\n",
    "# Initialize the parameters of the neural network\n",
    "# Bias terms are not necessary for word embeddings\n",
    "def init_weights(layer_dims): # (d_in, d1, d2, ..., d_L) -> [(A0,), (A1,), ..., (A_L,)]\n",
    "    N_layers = len(layer_dims)\n",
    "    params = []\n",
    "\n",
    "    for layer_idx in range(N_layers - 1): # Inbetween layers\n",
    "        # Get the dimensions of the current layer and the next layer\n",
    "        d_in = layer_dims[layer_idx]\n",
    "        d_out = layer_dims[layer_idx + 1]\n",
    "\n",
    "        # Weights should have mean=0 and variance=1/d_in, size = (d_out, d_in)\n",
    "        A = np.random.normal(0, 1/np.sqrt(d_in), size=(d_out, d_in))\n",
    "\n",
    "        params.append((A,))\n",
    "    return params\n",
    "\n",
    "# Count the number of parameters in the neural network\n",
    "def count_params(params):\n",
    "    n_params = 0\n",
    "    for layer_idx, (A,) in enumerate(params):\n",
    "        n_params += A.size \n",
    "    return n_params\n",
    "\n",
    "# Forward pass for batch of OHE words\n",
    "net_batch = jax.jit(jax.vmap(net, in_axes=(None, 0)))\n",
    "\n",
    "# Cross entropy loss\n",
    "@jax.jit\n",
    "def cross_entropy_loss(y, y_pred):\n",
    "    return -jnp.sum(y * jnp.log(y_pred))\n",
    "\n",
    "# Loss function for a batch of inputs\n",
    "@jax.jit\n",
    "def loss(params, \n",
    "         x, # batch inputs\n",
    "         y # batch outputs\n",
    "        ):\n",
    "    y_pred = net_batch(params, x)\n",
    "    y_pred = y_pred[..., 0]\n",
    "    return cross_entropy_loss(y, y_pred)\n",
    "\n",
    "# Compute the value and gradient of the loss function\n",
    "loss_value_and_grad = jax.jit(jax.value_and_grad(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7\n",
      "Number of parameters in the network: 28\n",
      "Layer 0 shape: (2, 7)\n",
      "Layer 1 shape: (7, 2)\n"
     ]
    }
   ],
   "source": [
    "# Try a small example\n",
    "word_to_idx, idx_to_word, word_count = build_vocab(filtered_sentences[:2])\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "V = vocab_size\n",
    "E = 2 # Embedding dimension\n",
    "\n",
    "params = init_weights([V, E, V])\n",
    "\n",
    "# let us count the number of parameters\n",
    "print(\"Number of parameters in the network:\", count_params(params))\n",
    "\n",
    "# Print the shapes of the weights \n",
    "for layer_idx, (A,) in enumerate(params):\n",
    "    print(f\"Layer {layer_idx} shape: {A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_training_data(filtered_sentences[:2], word_to_idx, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the input: (7, 1)\n",
      "shape of the output: (7, 1)\n",
      "output: [[0.06635585]\n",
      " [0.12693453]\n",
      " [0.23136471]\n",
      " [0.09541664]\n",
      " [0.11540101]\n",
      " [0.20822115]\n",
      " [0.15630615]]\n"
     ]
    }
   ],
   "source": [
    "# Single input text\n",
    "x_in = X[0][..., jnp.newaxis]\n",
    "print(f'shape of the input: {x_in.shape}')\n",
    "x_out = net(params, x_in)\n",
    "print(f'shape of the output: {x_out.shape}')\n",
    "print(f'output: {x_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the input: (2, 7, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[0.06635585],\n",
       "        [0.12693453],\n",
       "        [0.23136471],\n",
       "        [0.09541664],\n",
       "        [0.11540101],\n",
       "        [0.20822115],\n",
       "        [0.15630615]],\n",
       "\n",
       "       [[0.06635585],\n",
       "        [0.12693453],\n",
       "        [0.23136471],\n",
       "        [0.09541664],\n",
       "        [0.11540101],\n",
       "        [0.20822115],\n",
       "        [0.15630615]]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch input test\n",
    "x_batch = X[:2]\n",
    "x_batch = x_batch[..., jnp.newaxis]\n",
    "print(f'shape of the input: {x_batch.shape}')\n",
    "net_batch(params, x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimise the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "window_size = 8\n",
    "\n",
    "# Build the vocab\n",
    "sentences = filtered_stopwords_sentences\n",
    "word_to_idx, idx_to_word, word_count = build_vocab(sentences)\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Generate training data\n",
    "X, Y = generate_training_data(sentences, word_to_idx, window_size=window_size)\n",
    "print(f'X shape: {X.shape}')\n",
    "\n",
    "N = X.shape[0] # Number of training samples\n",
    "indices = jnp.arange(N) # Indices of the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 31600\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 10\n",
    "lr = 0.01\n",
    "embedding_size = 100\n",
    "batch_size = 128\n",
    "n_batches = N // batch_size\n",
    "\n",
    "# Define stopping criteria (loss tolerance)\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Initialize the parameters\n",
    "params = init_weights([vocab_size, embedding_size, vocab_size])\n",
    "print(f'Number of parameters: {count_params(params)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the loss and time\n",
    "loss_history = []\n",
    "time_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # shuffle the data\n",
    "    indices = np.random.permutation(indices)\n",
    "\n",
    "    # stores all the losses for this epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in range(n_batches):\n",
    "        # Get the current batch\n",
    "        batch_idices = indices[batch*batch_size:(batch+1)*batch_size]\n",
    "        x_batch = X[batch_idices]\n",
    "        y_batch = Y[batch_idices]\n",
    "\n",
    "        # Add a fake last dimension to the input\n",
    "        x_batch = x_batch[..., jnp.newaxis]\n",
    "\n",
    "        # Compute the loss and the gradient\n",
    "        loss_value, grad = loss_value_and_grad(params, x_batch, y_batch)\n",
    "        epoch_losses.append(loss_value)\n",
    "\n",
    "        # Update the parameters\n",
    "        params = jax.tree_util.tree_map(lambda p, g: p - lr * g, params, grad)\n",
    "\n",
    "    # Track the mean loss for each epoch\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    loss_history.append(mean_epoch_loss)\n",
    "    time_history.append(time.time())\n",
    "\n",
    "    # Check for convergence of loss\n",
    "    if epoch > 0 and abs(loss_history[-1] - loss_history[-2]) < tolerance:\n",
    "        print(f\"Stopping early at epoch {epoch+1} due to convergence of loss.\")\n",
    "        break\n",
    "\n",
    "    # Display the loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, loss = {mean_epoch_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the word embedding of a word\n",
    "def get_word_embedding(word, word_to_idx, word_embeddings):\n",
    "    idx = word_to_idx[word]\n",
    "    return word_embeddings[idx]\n",
    "\n",
    "# Function to compute the cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = jnp.dot(vec1, vec2)\n",
    "    norm_a = jnp.linalg.norm(vec1)\n",
    "    norm_b = jnp.linalg.norm(vec2)\n",
    "    if norm_a == 0 or norm_b == 0:  # Prevent division by zero\n",
    "        return 0.0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Function to evaluate similarities given a list of word pairs\n",
    "def evaluate_similarity(word_to_idx, word_embeddings, word_pairs):\n",
    "    scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in word_to_idx and word2 in word_to_idx:\n",
    "            vec1 = get_word_embedding(word1, word_to_idx, word_embeddings)\n",
    "            vec2 = get_word_embedding(word2, word_to_idx, word_embeddings)\n",
    "            cos_sim = cosine_similarity(vec1, vec2)\n",
    "            scores.append((word1, word2, cos_sim))\n",
    "        else:\n",
    "            print(f\"Warning: One of the words '{word1}' or '{word2}' is not in the vocabulary.\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embeddings: (158, 10)\n",
      "Warning: One of the words 'king' or 'queen' is not in the vocabulary.\n",
      "Warning: One of the words 'man' or 'woman' is not in the vocabulary.\n",
      "Warning: One of the words 'king' or 'man' is not in the vocabulary.\n",
      "Warning: One of the words 'apple' or 'fruit' is not in the vocabulary.\n",
      "Warning: One of the words 'mother' or 'father' is not in the vocabulary.\n",
      "Warning: One of the words 'son' or 'daugther' is not in the vocabulary.\n",
      "Warning: One of the words 'mother' or 'daughter' is not in the vocabulary.\n",
      "Cosine similarity between 'father' and 'son': -0.2851\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = params[0][0].T\n",
    "print(f\"Shape of word embeddings: {word_embeddings.shape}\")\n",
    "\n",
    "# Example word pairs and their expected similarities\n",
    "word_pairs = [\n",
    "    ('king', 'queen'),  # Expect high similarity\n",
    "    ('man', 'woman'),   # Expect high similarity\n",
    "    ('king', 'man'),    # Moderate similarity\n",
    "    ('apple', 'fruit'),  # Depending on your vocabulary, could expect a similarity\n",
    "\n",
    "    ('mother', 'father'),\n",
    "    ('son', 'daugther'),\n",
    "    ('father', 'son'),\n",
    "    ('mother', 'daughter')\n",
    "\n",
    "]\n",
    "\n",
    "# Run the evaluation\n",
    "similarity_scores = evaluate_similarity(word_to_idx, word_embeddings, word_pairs)\n",
    "\n",
    "# Print results\n",
    "for word1, word2, sim in similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scotty': 0,\n",
       " 'go': 1,\n",
       " 'back': 2,\n",
       " 'school': 3,\n",
       " 'parents': 4,\n",
       " 'talked': 5,\n",
       " 'seriously': 6,\n",
       " 'lengthily': 7,\n",
       " 'doctor': 8,\n",
       " 'specialist': 9,\n",
       " 'university': 10,\n",
       " 'hospital': 11,\n",
       " 'mckinley': 12,\n",
       " 'entitled': 13,\n",
       " 'discount': 14,\n",
       " 'members': 15,\n",
       " 'family': 16,\n",
       " 'decided': 17,\n",
       " 'would': 18,\n",
       " 'best': 19,\n",
       " 'take': 20,\n",
       " 'remainder': 21,\n",
       " 'term': 22,\n",
       " 'spend': 23,\n",
       " 'lot': 24,\n",
       " 'time': 25,\n",
       " 'bed': 26,\n",
       " 'rest': 27,\n",
       " 'pretty': 28,\n",
       " 'much': 29,\n",
       " 'chose': 30,\n",
       " 'provided': 31,\n",
       " 'course': 32,\n",
       " 'nothing': 33,\n",
       " 'exciting': 34,\n",
       " 'debilitating': 35,\n",
       " 'teacher': 36,\n",
       " 'principal': 37,\n",
       " 'conferred': 38,\n",
       " 'everyone': 39,\n",
       " 'agreed': 40,\n",
       " 'kept': 41,\n",
       " 'certain': 42,\n",
       " 'amount': 43,\n",
       " 'work': 44,\n",
       " 'home': 45,\n",
       " 'little': 46,\n",
       " 'danger': 47,\n",
       " 'losing': 48,\n",
       " 'accepted': 49,\n",
       " 'decision': 50,\n",
       " 'indifference': 51,\n",
       " 'enter': 52,\n",
       " 'arguments': 53,\n",
       " 'discharged': 54,\n",
       " 'checkup': 55,\n",
       " 'described': 56,\n",
       " 'celebration': 57,\n",
       " 'lunch': 58,\n",
       " 'cafeteria': 59,\n",
       " 'campus': 60,\n",
       " 'rachel': 61,\n",
       " 'wore': 62,\n",
       " 'smart': 63,\n",
       " 'hat': 64,\n",
       " 'warned': 65,\n",
       " 'recently': 66,\n",
       " 'smoking': 67,\n",
       " 'puffed': 68,\n",
       " 'cigarettes': 69,\n",
       " 'long': 70,\n",
       " 'ivory': 71,\n",
       " 'holder': 72,\n",
       " 'stained': 73,\n",
       " 'lipstick': 74,\n",
       " 'father': 75,\n",
       " 'sat': 76,\n",
       " 'sprawled': 77,\n",
       " 'chair': 78,\n",
       " 'angular': 79,\n",
       " 'alert': 80,\n",
       " 'cricket': 81,\n",
       " 'looking': 82,\n",
       " 'huge': 83,\n",
       " 'appointments': 84,\n",
       " 'room': 85,\n",
       " 'expression': 86,\n",
       " 'proprietorship': 87,\n",
       " 'teachers': 88,\n",
       " 'men': 89,\n",
       " 'brown': 90,\n",
       " 'suits': 91,\n",
       " 'gray': 92,\n",
       " 'hair': 93,\n",
       " 'pleasant': 94,\n",
       " 'smiles': 95,\n",
       " 'came': 96,\n",
       " 'table': 97,\n",
       " 'talk': 98,\n",
       " 'shop': 99,\n",
       " 'introduced': 100,\n",
       " 'polite': 101,\n",
       " 'indifferent': 102,\n",
       " 'ate': 103,\n",
       " 'food': 104,\n",
       " 'orange': 105,\n",
       " 'sauces': 106,\n",
       " 'gazed': 107,\n",
       " 'without': 108,\n",
       " 'interest': 109,\n",
       " 'heroic': 110,\n",
       " 'baronial': 111,\n",
       " 'windows': 112,\n",
       " 'bright': 113,\n",
       " 'ranks': 114,\n",
       " 'college': 115,\n",
       " 'banners': 116,\n",
       " 'tried': 117,\n",
       " 'make': 118,\n",
       " 'topic': 119,\n",
       " 'blueberry': 120,\n",
       " 'pie': 121,\n",
       " 'good': 122,\n",
       " 'recommend': 123,\n",
       " 'looked': 124,\n",
       " 'son': 125,\n",
       " 'face': 126,\n",
       " 'worried': 127,\n",
       " 'murmured': 128,\n",
       " 'thanks': 129,\n",
       " 'softly': 130,\n",
       " 'bend': 131,\n",
       " 'gaunt': 132,\n",
       " 'height': 133,\n",
       " 'across': 134,\n",
       " 'turn': 135,\n",
       " 'round': 136,\n",
       " 'ear': 137,\n",
       " 'regarded': 138,\n",
       " 'grizzled': 139,\n",
       " 'around': 140,\n",
       " 'moment': 141,\n",
       " 'said': 142,\n",
       " 'loudly': 143,\n",
       " 'full': 144,\n",
       " 'old': 145,\n",
       " 'pop': 146,\n",
       " 'eaten': 147,\n",
       " 'almost': 148,\n",
       " 'crested': 149,\n",
       " 'plate': 150,\n",
       " 'drunk': 151,\n",
       " 'half': 152,\n",
       " 'milk': 153,\n",
       " 'paper': 154,\n",
       " 'container': 155,\n",
       " 'right': 156,\n",
       " 'craig': 157}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-Gram with Negative Sampling (SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling simplifies this process by approximating the softmax function, \n",
    "focusing only on a few context words (positive samples) and a few randomly selected words from the vocabulary (negative samples) instead of the entire vocabulary.\n",
    "\n",
    "Loss Function: \\\n",
    "The loss function for Skip-Gram with Negative Sampling (SGNS) is typically a binary cross-entropy loss. For each target word $w_t$ and its context words $w_c$, the loss function tries to:\n",
    "- Maximize the probability that $w_c$ appears in the context of $w_t$\n",
    "- Minimize the probability that randomly selected (negative) words appear in the context\n",
    "\n",
    "The loss function is\n",
    "$$ L = - \\left[ \\sum_{w_c \\in \\text{context}} \\log \\sigma \\left( \\langle v_{w_c}, v_{w_t} \\rangle \\right) \n",
    "             +  \\sum_{w_n \\in \\text{negatives}} \\log \\sigma \\left( \\langle -v_{w_n}, v_{w_t} \\rangle \\right) \\right] $$\n",
    "\n",
    "where \n",
    "- $v_{w_t}$ is the embedding vector of target word $w_t$\n",
    "- $v_{w_c}$ is the embedding vector of context word $w_c$\n",
    "- $v_{w_n}$ is the embedding vector of negative word $w_n$ \n",
    "- $\\sigma(x)$ is the sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8155\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build vocabulary index\n",
    "def build_vocab_idx(filtered_sentences):\n",
    "    vocab_count = defaultdict(int)\n",
    "    for sentence in filtered_sentences:\n",
    "        for word in sentence:\n",
    "            vocab_count[word] += 1\n",
    "    return {word: idx for idx, (word, _) in enumerate(vocab_count.items())}, vocab_count\n",
    "\n",
    "vocab_idx, vocab_count = build_vocab_idx(filtered_sentences)\n",
    "vocab_size = len(vocab_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Context Words & Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: target word index in sentence, selected sentence, number of context words\n",
    "# output: list of context words index linked to vocab_idx, weights\n",
    "def get_context_words(target_idx, sentence, num_context_words):\n",
    "    window_size = num_context_words // 2\n",
    "    \n",
    "    start = max(0, target_idx - window_size)\n",
    "    end = min(len(sentence), target_idx + window_size + 1)\n",
    "\n",
    "    contexts = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        if i != target_idx:\n",
    "            contexts.append(vocab_idx[sentence[i]])\n",
    "            weights.append(1/abs(i-target_idx))\n",
    "    while len(contexts) < num_context_words:\n",
    "        contexts.append(vocab_size)\n",
    "    while len(weights) < num_context_words:\n",
    "        weights.append(vocab_size)\n",
    "    \n",
    "    return contexts, weights\n",
    "\n",
    "\n",
    "# input: target word index in vocab_idx, number of unique words, number of negative samples, contexts output of get_context_words()\n",
    "# output: list of negative words index linked to vocab_idx\n",
    "def get_negative_samples(target, vocab_size, num_negative_samples, contexts):\n",
    "    negative_samples = set()\n",
    "    \n",
    "    while len(negative_samples) < num_negative_samples:\n",
    "        # Sample a random word from the vocabulary\n",
    "        neg_sample = np.random.randint(0, vocab_size)\n",
    "        # Ensure it's not the target word or context word\n",
    "        if neg_sample != target and neg_sample not in contexts:\n",
    "            negative_samples.add(neg_sample)\n",
    "    \n",
    "    return list(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Descent & Backtracking Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(fn, initial_lr, beta, eta, W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights):\n",
    "    # Convert to array for it to work with jit.vmap\n",
    "    batch_targets = jnp.array(batch_targets)\n",
    "    batch_contexts = jnp.array(batch_contexts)\n",
    "    batch_neg_samples = jnp.array(batch_neg_samples)\n",
    "    batch_weights = jnp.array(batch_weights)\n",
    "    \n",
    "    # Get initial loss and gradient\n",
    "    initial_loss = fn(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "    gradients = jax.grad(fn, argnums=(0, 1))(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "\n",
    "    # Backtracking line search\n",
    "    # Descent Direction for W_in and W_out\n",
    "    d_in, d_out = -gradients[0], -gradients[1]\n",
    "    lr = initial_lr\n",
    "\n",
    "    while True:\n",
    "        new_W_in = W_in + lr * d_in\n",
    "        new_W_out = W_out + lr * d_out\n",
    "        new_loss = fn(new_W_in, new_W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "        \n",
    "        # Check if the new loss meets the sufficient decrease condition\n",
    "        # Exit loop if condition is met\n",
    "        if new_loss <= initial_loss + eta * lr * (jnp.sum(gradients[0] * d_in) + jnp.sum(gradients[1] * d_out)):\n",
    "            break\n",
    "\n",
    "        # Reduce alpha if condition is not met\n",
    "        lr *= beta\n",
    "\n",
    "    # Update embeddings\n",
    "    W_in += lr * d_in\n",
    "    W_out += lr * d_out\n",
    "\n",
    "    return W_in, W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: W_in, W_out, target word index in vocab_idx, contexts & weights output of get_context_words(), \n",
    "#   negative_samples output of get_negative_samples()\n",
    "# output: loss value\n",
    "@jax.jit\n",
    "def SGNS_single_loss_fn(W_in, W_out, target, contexts, negative_samples, weights):\n",
    "    # Get word embeddings\n",
    "    target_embedding = W_in[target]\n",
    "\n",
    "    pos_loss = 0\n",
    "    neg_loss = 0\n",
    "    \n",
    "    # Positive score\n",
    "    for idx, i in enumerate(contexts):\n",
    "        weight = weights[idx]\n",
    "        context_embedding = W_out[i]\n",
    "        positive_score = jnp.dot(context_embedding, target_embedding)\n",
    "        # Add 1e-10 to prevent log underflow\n",
    "        pos_loss += jnp.log(jax.nn.sigmoid(positive_score) + 1e-10) * weight\n",
    "\n",
    "    # Negative scores and loss\n",
    "    for j in negative_samples:\n",
    "        negative_embedding = W_out[j]\n",
    "        negative_scores = jnp.dot(-negative_embedding, target_embedding)\n",
    "        # Add 1e-10 to prevent log underflow\n",
    "        neg_loss += jnp.log(jax.nn.sigmoid(negative_scores) + 1e-10)\n",
    "\n",
    "    # Total loss is the sum of positive loss and negative loss\n",
    "    return -(pos_loss + neg_loss)\n",
    "\n",
    "SGNS_loss_batched = jax.jit(jax.vmap(SGNS_single_loss_fn, in_axes=(None, None, 0, 0, 0, 0)))\n",
    "\n",
    "@jax.jit\n",
    "def SGNS_batched_loss_fn(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights):\n",
    "    individual_losses = SGNS_loss_batched(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "    return jnp.mean(individual_losses)\n",
    "\n",
    "SGNS_batched_loss_fn_jit = jax.jit(SGNS_batched_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "embedding_dim = 100\n",
    "num_negative_samples = 5\n",
    "num_context_words = 4\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "\n",
    "initial_lr = 1.0\n",
    "beta = 0.5\n",
    "eta = 0.1\n",
    "\n",
    "# Initiate W_in (for target) and W_out (for context and negative words) matrix\n",
    "# Add a 0 row in last row for target words with less than number of context words required e.g words as start & end of sentence\n",
    "W_in = jax.random.normal(jax.random.PRNGKey(0), (vocab_size, embedding_dim)) * 0.01\n",
    "W_in = jnp.vstack([W_in, jnp.zeros((1, embedding_dim))])\n",
    "W_out = jax.random.normal(jax.random.PRNGKey(1), (vocab_size, embedding_dim)) * 0.01\n",
    "W_out = jnp.vstack([W_out, jnp.zeros((1, embedding_dim))])\n",
    "\n",
    "context_dict = {}\n",
    "weight_dict = {}\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # Initiate for batch\n",
    "    batch_targets, batch_contexts, batch_neg_samples, batch_weights = [], [], [], []\n",
    "    for sentence in filtered_stopwords_sentences:\n",
    "        for target_idx, word in enumerate(sentence):\n",
    "            target = vocab_idx[word]\n",
    "            \n",
    "            # Save context words and weights for future epoch\n",
    "            if epoch > 0:\n",
    "                contexts = context_dict[target]\n",
    "                weights = weight_dict[target]\n",
    "            else:\n",
    "                contexts, weights = get_context_words(target_idx, sentence, num_context_words)\n",
    "                context_dict[target] = contexts\n",
    "                weight_dict[target] = weights\n",
    "\n",
    "            # Get negative words\n",
    "            negative_samples = get_negative_samples(target, vocab_size, num_negative_samples, contexts)\n",
    "\n",
    "            # Add target, context, negative_samples, weights for batch\n",
    "            # Batch Targets Shape: (batch_size,)\n",
    "            batch_targets.append(target)\n",
    "            # Batch Contexts Shape: (batch_size, num_context_words)\n",
    "            batch_contexts.append(contexts)\n",
    "            # Batch Negative Samples Shape: (batch_size, num_negative_samples)\n",
    "            batch_neg_samples.append(negative_samples)\n",
    "            # Batch Weights Shape: (batch_size, num_context_words)\n",
    "            batch_weights.append(weights)\n",
    "\n",
    "            if len(batch_targets) == batch_size:\n",
    "                W_in, W_out = \\\n",
    "                    grad_descent(SGNS_batched_loss_fn_jit, initial_lr, beta, eta,\\\n",
    "                        W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "\n",
    "                # Reset batches\n",
    "                batch_targets, batch_contexts, batch_neg_samples, batch_weights = [], [], [], []\n",
    "    \n",
    "    # Include last few words not enough to form batch\n",
    "    if len(batch_targets) != 0:\n",
    "        W_in, W_out = \\\n",
    "            grad_descent(SGNS_batched_loss_fn_jit, initial_lr, beta, eta,\\\n",
    "                W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: One of the words 'apple' or 'fruit' is not in the vocabulary.\n",
      "Cosine similarity between 'king' and 'queen': 0.7433\n",
      "Cosine similarity between 'man' and 'woman': 0.8463\n",
      "Cosine similarity between 'king' and 'man': -0.8466\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# Function to compute cosine similarity using JAX\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = jnp.dot(vec1, vec2)\n",
    "    norm_a = jnp.linalg.norm(vec1)\n",
    "    norm_b = jnp.linalg.norm(vec2)\n",
    "    if norm_a == 0 or norm_b == 0:  # Prevent division by zero\n",
    "        return 0.0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Example word pairs and their expected similarities\n",
    "word_pairs = [\n",
    "    ('king', 'queen'),  # Expect high similarity\n",
    "    ('man', 'woman'),   # Expect high similarity\n",
    "    ('king', 'man'),    # Moderate similarity\n",
    "    ('apple', 'fruit'),  # Depending on your vocabulary, could expect a similarity\n",
    "]\n",
    "\n",
    "# Function to evaluate similarities using W_out\n",
    "def evaluate_similarity(vocab_idx, W_out, word_pairs):\n",
    "    scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in vocab_idx and word2 in vocab_idx:\n",
    "            vec1 = W_out[vocab_idx[word1]]  # Get the output embedding for word1\n",
    "            vec2 = W_out[vocab_idx[word2]]  # Get the output embedding for word2\n",
    "            cos_sim = cosine_similarity(vec1, vec2)\n",
    "            scores.append((word1, word2, cos_sim))\n",
    "        else:\n",
    "            print(f\"Warning: One of the words '{word1}' or '{word2}' is not in the vocabulary.\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Run the evaluation\n",
    "similarity_scores = evaluate_similarity(vocab_idx, W_out, word_pairs)\n",
    "\n",
    "# Print results\n",
    "for word1, word2, sim in similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Skip-Gram with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Softmax is an alternative to Negative Sampling, where the softmax function is approximated using a binary tree. The loss function is structured similarly to SGNS but involves traversing the binary tree structure and computing a conditional probability at each node.\n",
    "\n",
    "In this case, the loss function minimizes the difference between the predicted probability and the actual label for each word pair, traversing the tree for each prediction. The key idea is to reduce the computational complexity of softmax when dealing with a large vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CBOW, the objective is to predict the target word $w_t$ given the context words $w_c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  CBOW with Negative Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes the average of the context word embeddings and tries to predict the target word. The loss function for CBOW with Negative Sampling is also binary cross-entropy but formulated slightly differently due to the averaging of context word embeddings.\n",
    "\n",
    "The loss function is\n",
    "$$ L = - \\left[ \\log \\sigma \\left( \\langle v_{w_t}, v_C \\rangle \\right) \n",
    "             +  \\sum_{w_n \\in \\text{negatives}} \\log \\sigma \\left( \\langle -v_{w_n}, v_C \\rangle \\right) \\right] $$  \n",
    "where \n",
    "$$ v_C = \\frac{1}{|C|} \\sum_{w_c \\in C} v_{w_c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: W_in, W_out, target word index in vocab_idx, contexts & weights output of get_context_words(), \n",
    "#   negative_samples output of get_negative_samples()\n",
    "# output: loss value\n",
    "@jax.jit\n",
    "def CBOW_single_loss_fn(W_in, W_out, target, contexts, negative_samples, weights):\n",
    "    # Get word embeddings\n",
    "    target_embedding = W_in[target]\n",
    "\n",
    "    neg_loss = 0\n",
    "    \n",
    "    # Get mean context vector\n",
    "    context_total = jnp.zeros((1, embedding_dim))\n",
    "    for idx, i in enumerate(contexts):\n",
    "        weight = weights[idx]\n",
    "        context_embedding = W_out[i]\n",
    "        context_total += weight * context_embedding\n",
    "    context_mean = context_total/(idx+1)\n",
    "    \n",
    "    # Positive score\n",
    "    positive_score = jnp.dot(context_mean, target_embedding)\n",
    "    pos_loss = jnp.log(jax.nn.sigmoid(positive_score)) * weight\n",
    "\n",
    "    # Negative scores and loss\n",
    "    for j in negative_samples:\n",
    "        negative_embedding = W_out[j]\n",
    "        negative_scores = jnp.dot(context_mean, -negative_embedding)\n",
    "        neg_loss += jnp.log(jax.nn.sigmoid(negative_scores))\n",
    "\n",
    "    # Total loss is the sum of positive loss and negative loss\n",
    "    return -(pos_loss + neg_loss)\n",
    "\n",
    "CBOW_loss_batched = jax.jit(jax.vmap(CBOW_single_loss_fn, in_axes=(None, None, 0, 0, 0, 0)))\n",
    "\n",
    "@jax.jit\n",
    "def CBOW_batched_loss_fn(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights):\n",
    "    individual_losses = CBOW_loss_batched(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "    return jnp.mean(individual_losses)\n",
    "\n",
    "CBOW_batched_loss_fn_jit = jax.jit(CBOW_batched_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "embedding_dim = 100\n",
    "num_negative_samples = 5\n",
    "num_context_words = 4\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "\n",
    "initial_lr = 1.0\n",
    "beta = 0.5\n",
    "eta = 0.1\n",
    "\n",
    "# Initiate W_in (for target) and W_out (for context and negative words) matrix\n",
    "# Add a 0 row in last row for target words with less than number of context words required e.g words as start & end of sentence\n",
    "W_in = jax.random.normal(jax.random.PRNGKey(0), (vocab_size, embedding_dim)) * 0.01\n",
    "W_in = jnp.vstack([W_in, jnp.zeros((1, embedding_dim))])\n",
    "W_out = jax.random.normal(jax.random.PRNGKey(1), (vocab_size, embedding_dim)) * 0.01\n",
    "W_out = jnp.vstack([W_out, jnp.zeros((1, embedding_dim))])\n",
    "\n",
    "context_dict = {}\n",
    "weight_dict = {}\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # Initiate for batch\n",
    "    batch_targets, batch_contexts, batch_neg_samples, batch_weights = [], [], [], []\n",
    "    for sentence in filtered_stopwords_sentences:\n",
    "        for target_idx, word in enumerate(sentence):\n",
    "            target = vocab_idx[word]\n",
    "            \n",
    "            # Save context words and weights for future epoch\n",
    "            if epoch > 0:\n",
    "                contexts = context_dict[target]\n",
    "                weights = weight_dict[target]\n",
    "            else:\n",
    "                contexts, weights = get_context_words(target_idx, sentence, num_context_words)\n",
    "                context_dict[target] = contexts\n",
    "                weight_dict[target] = weights\n",
    "\n",
    "            # Get negative words\n",
    "            negative_samples = get_negative_samples(target, vocab_size, num_negative_samples, contexts)\n",
    "\n",
    "            # Add target, context, negative_samples, weights for batch\n",
    "            # Batch Targets Shape: (batch_size,)\n",
    "            batch_targets.append(target)\n",
    "            # Batch Contexts Shape: (batch_size, num_context_words)\n",
    "            batch_contexts.append(contexts)\n",
    "            # Batch Negative Samples Shape: (batch_size, num_negative_samples)\n",
    "            batch_neg_samples.append(negative_samples)\n",
    "            # Batch Weights Shape: (batch_size, num_context_words)\n",
    "            batch_weights.append(weights)\n",
    "\n",
    "            if len(batch_targets) == batch_size:\n",
    "                W_in, W_out = \\\n",
    "                    grad_descent(CBOW_batched_loss_fn_jit, initial_lr, beta, eta,\\\n",
    "                        W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "\n",
    "                # Reset batches\n",
    "                batch_targets, batch_contexts, batch_neg_samples, batch_weights = [], [], [], []\n",
    "    \n",
    "    # Include last few words not enough to form batch\n",
    "    if len(batch_targets) != 0:\n",
    "        W_in, W_out = \\\n",
    "            grad_descent(CBOW_batched_loss_fn_jit, initial_lr, beta, eta,\\\n",
    "                W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: One of the words 'apple' or 'fruit' is not in the vocabulary.\n",
      "Cosine similarity between 'king' and 'queen': -0.0612\n",
      "Cosine similarity between 'man' and 'woman': 0.0513\n",
      "Cosine similarity between 'king' and 'man': 0.1846\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# Function to compute cosine similarity using JAX\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = jnp.dot(vec1, vec2)\n",
    "    norm_a = jnp.linalg.norm(vec1)\n",
    "    norm_b = jnp.linalg.norm(vec2)\n",
    "    if norm_a == 0 or norm_b == 0:  # Prevent division by zero\n",
    "        return 0.0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Example word pairs and their expected similarities\n",
    "word_pairs = [\n",
    "    ('king', 'queen'),  # Expect high similarity\n",
    "    ('man', 'woman'),   # Expect high similarity\n",
    "    ('king', 'man'),    # Moderate similarity\n",
    "    ('apple', 'fruit'),  # Depending on your vocabulary, could expect a similarity\n",
    "]\n",
    "\n",
    "# Function to evaluate similarities using W_out\n",
    "def evaluate_similarity(vocab_idx, W_out, word_pairs):\n",
    "    scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in vocab_idx and word2 in vocab_idx:\n",
    "            vec1 = W_out[vocab_idx[word1]]  # Get the output embedding for word1\n",
    "            vec2 = W_out[vocab_idx[word2]]  # Get the output embedding for word2\n",
    "            cos_sim = cosine_similarity(vec1, vec2)\n",
    "            scores.append((word1, word2, cos_sim))\n",
    "        else:\n",
    "            print(f\"Warning: One of the words '{word1}' or '{word2}' is not in the vocabulary.\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Run the evaluation\n",
    "similarity_scores = evaluate_similarity(vocab_idx, W_out, word_pairs)\n",
    "\n",
    "# Print results\n",
    "for word1, word2, sim in similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Skip-Gram, CBOW can also use hierarchical softmax as an alternative to Negative Sampling, traversing a binary tree to approximate the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all words, remove all punctuations and stop words\n",
    "def preprocess_sentence(sentence):\n",
    "    return [word.lower() for word in sentence if word.lower() not in stopwords and word not in punctuation]\n",
    "\n",
    "corpus_preprocessed = [preprocess_sentence(sentence) for sentence in corpus]\n",
    "corpus_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Window size for Co-occurrence matrix - 2 words before, 2 words after\n",
    "window_size = 2\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "def build_cooccurrence_matrix(corpus_preprocessed, vocab_idx, window_size):\n",
    "    cooccurrence_matrix = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for sentence in corpus_preprocessed:\n",
    "        sentence_length = len(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            word_idx = vocab_idx[word]\n",
    "            # Context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(sentence_length, i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                # Skip target word\n",
    "                if i != j:\n",
    "                    context_word = sentence[j]\n",
    "                    context_word_idx = vocab_idx[context_word]\n",
    "                    # Increment the co-occurrence count with inverse distance weighting\n",
    "                    cooccurrence_matrix[word_idx][context_word_idx] += 1.0 / abs(i - j)\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(corpus_preprocessed, vocab_idx, window_size)\n",
    "cooccurrence_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "embedding_dim = 50\n",
    "\n",
    "# Initialize word vectors and bias\n",
    "key = jax.random.PRNGKey(0)\n",
    "v = jax.random.normal(key, (vocab_size, embedding_dim))\n",
    "key, subkey = jax.random.split(key)\n",
    "v_tilde = jax.random.normal(subkey, (vocab_size, embedding_dim))\n",
    "bias = jnp.zeros(vocab_size)\n",
    "bias_tilde = jnp.zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight function\n",
    "def f(x):\n",
    "    return jax.minimum(1.0, (x/x_max)**alpha)\n",
    "\n",
    "# loss function\n",
    "def gLove_loss(v, v_tilde, bias, bias_tilde, cooccurrence_matrix):\n",
    "    loss = 0.0\n",
    "    for word in cooccurrence_matrix:\n",
    "        for context_word in cooccurrence_matrix[word]:\n",
    "            X_ij = cooccurrence_matrix[word][context_word]\n",
    "            weight = f(X_ij)\n",
    "            diff = jnp.dot(v[word], v_tilde[context_word]) + bias[word] + bias_tilde[context_word] - jnp.log(X_ij)\n",
    "            loss += weight * diff**2\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
