{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Implement a simple word embedding in Python (from scratch) and use it to find the most similar words to a given word. Come up with a dataset and evaluation metrics to evaluate the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK's stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"she's\", 'herself', 'further', 're', 'whom', 'because', 'didn', 'such', 'his', 'out', 'those', 'doesn', 'm', 'all', 'be', 'each', 'theirs', 'the', 'while', 'and', \"it's\", 'been', 'them', 'under', 't', 'just', 'him', 'does', 'there', 'they', \"hadn't\", 'itself', 'o', 'over', 'below', 'than', 'if', 'what', 's', 'against', \"hasn't\", 'during', 'mightn', 'can', 'have', 'hadn', 'ma', 'don', 'through', 'once', 'in', 'only', \"couldn't\", 'why', 'yourself', 'between', \"won't\", 'himself', 'more', 'hasn', 'into', 'with', 'shan', 'when', 'up', 'where', \"needn't\", 'who', 'about', 'should', 'doing', 'to', 'both', 'he', 'off', 'some', \"mustn't\", 'needn', 'aren', 'so', 'an', 'had', 'y', 'few', \"haven't\", 'down', \"shouldn't\", 'ours', 'own', 'i', 'me', \"didn't\", 'now', 'too', 'then', 'from', 'its', 'again', \"weren't\", 'shouldn', 'd', 'which', 'ourselves', 'wasn', 'hers', 'or', 'will', 've', \"should've\", 'very', 'but', 'her', 'at', 'our', 'it', 'wouldn', 'for', 'ain', 'until', 'their', 'a', 'my', 'that', 'after', 'how', \"you've\", 'most', 'here', \"don't\", 'having', \"you're\", 'same', \"you'd\", 'isn', 'couldn', \"doesn't\", 'do', 'themselves', \"mightn't\", 'any', 'nor', \"you'll\", \"aren't\", 'she', 'being', 'myself', 'did', 'haven', 'was', 'not', 'of', 'your', 'this', \"shan't\", 'we', 'these', 'by', 'before', 'above', 'as', 'mustn', 'yours', 'on', 'weren', 'other', 'll', 'are', 'yourselves', \"wouldn't\", 'you', 'has', \"that'll\", 'no', 'were', 'am', \"isn't\", 'won', \"wasn't\", 'is'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\") # Uncomment this line to download the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Brown Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"brown\") # Uncomment this line to download the brown corpus\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the categories of the brown corpus \n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4249\n",
      "Sentence 0: ['Thirty-three']\n",
      "Sentence 1: ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.']\n",
      "Sentence 2: ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.']\n",
      "Sentence 3: ['His', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.']\n",
      "Sentence 4: ['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.']\n"
     ]
    }
   ],
   "source": [
    "# Get the sentences from the \"fiction\" category\n",
    "sentences = brown.sents(categories=\"fiction\")\n",
    "\n",
    "# Number of sentences\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i}: {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'did', 'not', 'go', 'back', 'to', 'school']\n",
      "Filtered sentence 2: ['his', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'university', 'hospital', 'mckinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', 'for', 'the', 'rest', 'do', 'pretty', 'much', 'as', 'he', 'chose', 'provided', 'of', 'course', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating']\n",
      "Filtered sentence 3: ['his', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-alphanumeric words and convert to lowercase\n",
    "filtered_sentences = []\n",
    "\n",
    "for sent in sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum()]\n",
    "    filtered_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'go', 'back', 'school']\n",
      "Filtered sentence 2: ['parents', 'talked', 'seriously', 'lengthily', 'doctor', 'specialist', 'university', 'hospital', 'mckinley', 'entitled', 'discount', 'members', 'family', 'decided', 'would', 'best', 'take', 'remainder', 'term', 'spend', 'lot', 'time', 'bed', 'rest', 'pretty', 'much', 'chose', 'provided', 'course', 'chose', 'nothing', 'exciting', 'debilitating']\n",
      "Filtered sentence 3: ['teacher', 'school', 'principal', 'conferred', 'everyone', 'agreed', 'kept', 'certain', 'amount', 'work', 'home', 'little', 'danger', 'losing', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'decision', 'indifference', 'enter', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords and non-alphanumeric words and convert to lowercase\n",
    "filtered_stopwords_sentences = []\n",
    "\n",
    "for sent in filtered_sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum() and word.lower() not in stopwords]\n",
    "    filtered_stopwords_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_stopwords_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-Gram with Negative Sampling (SGNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Skip-Gram with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  CBOW with Negative Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
