{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Implement a simple word embedding in Python (from scratch) and use it to find the most similar words to a given word. Come up with a dataset and evaluation metrics to evaluate the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK's stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'those', 'no', 'ourselves', 'very', \"you'd\", 'any', \"doesn't\", 'our', 'mightn', 'i', 'shouldn', 're', 'each', 'will', 'now', \"haven't\", 'o', 'you', 's', \"didn't\", 'off', 'don', 'too', 'd', 'did', 'below', \"shouldn't\", 'that', 'isn', \"aren't\", 'during', 'theirs', 'can', 'just', 'as', 'once', 'hers', 'not', 'whom', 'or', 'again', 'yourselves', 'its', 'who', 'there', 'if', \"hadn't\", 'them', 'needn', 'before', 'll', 'weren', 'themselves', 'because', \"you'll\", 'his', 've', 'haven', \"you're\", 'against', 'when', 'be', 'm', 'herself', 'over', 'own', \"wouldn't\", 'these', 'they', 'aren', 'y', \"needn't\", 'for', 'up', 'about', 'yours', 'being', 'how', 'both', 'which', 'a', 'am', 'couldn', 'here', 'this', 'are', 'doing', 'from', 'with', 'is', \"you've\", 'some', 'won', \"that'll\", \"won't\", 'what', 'more', 'hasn', 'we', 'mustn', 'after', 'other', 'yourself', 'was', 'in', 'wasn', 'itself', 'by', \"mustn't\", 'had', 'she', 'so', 'such', 'has', 'me', 'further', 'and', 'he', \"she's\", \"it's\", 'the', 'my', 'to', 'where', 'shan', 'than', 'doesn', \"don't\", 'out', 'at', \"shan't\", 'most', 'nor', 'their', 'hadn', \"isn't\", 'of', 'under', \"mightn't\", \"should've\", 'should', 'all', \"couldn't\", 'didn', 'myself', 'between', 'an', 'on', 'down', 'above', 'same', \"wasn't\", \"weren't\", 'until', 'having', 'have', 'ours', 'then', 'it', 'been', 'but', 'why', 'ma', 't', 'your', 'were', 'wouldn', 'him', 'himself', 'do', 'her', 'into', 'few', \"hasn't\", 'does', 'while', 'ain', 'through', 'only'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\") # Uncomment this line to download the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Brown Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"brown\") # Uncomment this line to download the brown corpus\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the categories of the brown corpus \n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4249\n",
      "Sentence 0: ['Thirty-three']\n",
      "Sentence 1: ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.']\n",
      "Sentence 2: ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.']\n",
      "Sentence 3: ['His', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.']\n",
      "Sentence 4: ['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.']\n"
     ]
    }
   ],
   "source": [
    "# Get the sentences from the \"fiction\" category\n",
    "sentences = brown.sents(categories=\"fiction\")\n",
    "\n",
    "# Number of sentences\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i}: {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'did', 'not', 'go', 'back', 'to', 'school']\n",
      "Filtered sentence 2: ['his', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'university', 'hospital', 'mckinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', 'for', 'the', 'rest', 'do', 'pretty', 'much', 'as', 'he', 'chose', 'provided', 'of', 'course', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating']\n",
      "Filtered sentence 3: ['his', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-alphanumeric words and convert to lowercase\n",
    "filtered_sentences = []\n",
    "\n",
    "for sent in sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum()]\n",
    "    filtered_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence 0: []\n",
      "Filtered sentence 1: ['scotty', 'go', 'back', 'school']\n",
      "Filtered sentence 2: ['parents', 'talked', 'seriously', 'lengthily', 'doctor', 'specialist', 'university', 'hospital', 'mckinley', 'entitled', 'discount', 'members', 'family', 'decided', 'would', 'best', 'take', 'remainder', 'term', 'spend', 'lot', 'time', 'bed', 'rest', 'pretty', 'much', 'chose', 'provided', 'course', 'chose', 'nothing', 'exciting', 'debilitating']\n",
      "Filtered sentence 3: ['teacher', 'school', 'principal', 'conferred', 'everyone', 'agreed', 'kept', 'certain', 'amount', 'work', 'home', 'little', 'danger', 'losing', 'term']\n",
      "Filtered sentence 4: ['scotty', 'accepted', 'decision', 'indifference', 'enter', 'arguments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords and non-alphanumeric words and convert to lowercase\n",
    "filtered_stopwords_sentences = []\n",
    "\n",
    "for sent in filtered_sentences:\n",
    "    filtered_sent = [word.lower() for word in sent if word.isalnum() and word.lower() not in stopwords]\n",
    "    filtered_stopwords_sentences.append(filtered_sent)\n",
    "\n",
    "# Print the first 5 sentences\n",
    "for i in range(5):\n",
    "    print(f\"Filtered sentence {i}: {filtered_stopwords_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Skip-Gram, the goal is to predict the context words $w_c$ given a target word $w_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-Gram with Negative Sampling (SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling simplifies this process by approximating the softmax function, \n",
    "focusing only on a few context words (positive samples) and a few randomly selected words from the vocabulary (negative samples) instead of the entire vocabulary.\n",
    "\n",
    "Loss Function: \\\n",
    "The loss function for Skip-Gram with Negative Sampling (SGNS) is typically a binary cross-entropy loss. For each target word $w_t$ and its context words $w_c$, the loss function tries to:\n",
    "- Maximize the probability that $w_c$ appears in the context of $w_t$\n",
    "- Minimize the probability that randomly selected (negative) words appear in the context\n",
    "\n",
    "The loss function is\n",
    "$$ L = - \\left[ \\sum_{w_c \\in \\text{context}} \\log \\sigma \\left( \\langle v_{w_c}, v_{w_t} \\rangle \\right) \n",
    "             +  \\sum_{w_n \\in \\text{negatives}} \\log \\sigma \\left( \\langle -v_{w_n}, v_{w_t} \\rangle \\right) \\right] $$\n",
    "\n",
    "where \n",
    "- $v_{w_t}$ is the embedding vector of target word $w_t$\n",
    "- $v_{w_c}$ is the embedding vector of context word $w_c$\n",
    "- $v_{w_n}$ is the embedding vector of negative word $w_n$ \n",
    "- $\\sigma(x)$ is the sigmoid function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build vocabulary index\n",
    "def build_vocab_idx(filtered_sentences):\n",
    "    vocab_count = defaultdict(int)\n",
    "    for sentence in filtered_sentences:\n",
    "        for word in sentence:\n",
    "            vocab_count[word] += 1\n",
    "    return {word: idx for idx, (word, _) in enumerate(vocab_count.items())}, vocab_count\n",
    "\n",
    "embedding_dim = 100 \n",
    "vocab_idx, vocab_count = build_vocab_idx(filtered_sentences)\n",
    "vocab_size = len(vocab_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Context Words & Negative Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: target word index in sentence, selected sentence, number of context words\n",
    "# output: list of context words index linked to vocab_idx, weights\n",
    "def get_context_words(target_idx, sentence, num_context_words):\n",
    "    window_size = num_context_words // 2\n",
    "    \n",
    "    start = max(0, target_idx - window_size)\n",
    "    end = min(len(sentence), target_idx + window_size + 1)\n",
    "\n",
    "    contexts = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(start, end):\n",
    "        if i != target_idx:\n",
    "            contexts.append(vocab_idx[sentence[i]])\n",
    "            weights.append(1/abs(i-target_idx))\n",
    "    while len(contexts) < num_context_words:\n",
    "        contexts.append(vocab_size)\n",
    "    while len(weights) < num_context_words:\n",
    "        weights.append(vocab_size)\n",
    "    \n",
    "    return contexts, weights\n",
    "\n",
    "\n",
    "# input: target word index in vocab_idx, number of unique words, number of negative samples, contexts output of get_context_words()\n",
    "# output: list of negative words index linked to vocab_idx\n",
    "def get_negative_samples(target, vocab_size, num_negative_samples, contexts):\n",
    "    negative_samples = set()\n",
    "    \n",
    "    while len(negative_samples) < num_negative_samples:\n",
    "        # Sample a random word from the vocabulary\n",
    "        neg_sample = np.random.randint(0, vocab_size)\n",
    "        # Ensure it's not the target word or context word\n",
    "        if neg_sample != target and neg_sample not in contexts:\n",
    "            negative_samples.add(neg_sample)\n",
    "    \n",
    "    return list(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: W_in, W_out, target word index in vocab_idx, contexts & weights output of get_context_words(), \n",
    "#   negative_samples output of get_negative_samples()\n",
    "# output: loss value\n",
    "@jax.jit\n",
    "def single_loss_fn(W_in, W_out, target, contexts, negative_samples, weights):\n",
    "    # Get word embeddings\n",
    "    target_embedding = W_in[target]\n",
    "\n",
    "    pos_loss = 0\n",
    "    neg_loss = 0\n",
    "    \n",
    "    # Positive score\n",
    "    for idx, i in enumerate(contexts):\n",
    "        weight = weights[idx]\n",
    "        context_embedding = W_out[i]\n",
    "        positive_score = jnp.dot(context_embedding, target_embedding)\n",
    "        pos_loss += jnp.log(jax.nn.sigmoid(positive_score)) * weight\n",
    "\n",
    "    # Negative scores and loss\n",
    "    for j in negative_samples:\n",
    "        negative_embedding = W_out[j]\n",
    "        negative_scores = jnp.dot(-negative_embedding, target_embedding)\n",
    "        neg_loss += jnp.log(jax.nn.sigmoid(-negative_scores))\n",
    "\n",
    "    # Total loss is the sum of positive loss and negative loss\n",
    "    return -(pos_loss + neg_loss)\n",
    "\n",
    "loss_batched = jax.jit(jax.vmap(single_loss_fn, in_axes=(None, None, 0, 0, 0, 0)))\n",
    "\n",
    "@jax.jit\n",
    "def batched_loss_fn(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights):\n",
    "    individual_losses = loss_batched(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "    return jnp.mean(individual_losses)\n",
    "\n",
    "batched_loss_fn_jit = jax.jit(batched_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_negative_samples = 5\n",
    "num_context_words = 4\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "\n",
    "initial_lr = 1.0\n",
    "beta = 0.5\n",
    "eta = 0.1\n",
    "\n",
    "# Initiate W_in (for target) and W_out (for context and negative words) matrix\n",
    "# Add a 0 row in last row for target words with less than number of context words required e.g words as start & end of sentence\n",
    "W_in = jax.random.normal(jax.random.PRNGKey(0), (vocab_size, embedding_dim)) * 0.01\n",
    "W_in = jnp.vstack([W_in, jnp.zeros((1, embedding_dim))])\n",
    "W_out = jax.random.normal(jax.random.PRNGKey(1), (vocab_size, embedding_dim)) * 0.01\n",
    "W_out = jnp.vstack([W_out, jnp.zeros((1, embedding_dim))])\n",
    "\n",
    "context_dict = {}\n",
    "weight_dict = {}\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # Initiate for batch\n",
    "    batch_targets, batch_contexts, batch_neg_samples, batch_weights = [], [], [], []\n",
    "    for sentence in filtered_stopwords_sentences:\n",
    "        for target_idx, word in enumerate(sentence):\n",
    "            target = vocab_idx[word]\n",
    "            \n",
    "            # Save context words and weights for future epoch\n",
    "            if epoch > 0:\n",
    "                contexts = context_dict[target]\n",
    "                weights = weight_dict[target]\n",
    "            else:\n",
    "                contexts, weights = get_context_words(target_idx, sentence, num_context_words)\n",
    "                context_dict[target] = contexts\n",
    "                weight_dict[target] = weights\n",
    "\n",
    "            # Get negative words\n",
    "            negative_samples = get_negative_samples(target, vocab_size, num_negative_samples, contexts)\n",
    "\n",
    "            # Add target, context, negative_samples, weights for batch\n",
    "            # Batch Targets Shape: (batch_size,)\n",
    "            batch_targets.append(target)\n",
    "            # Batch Contexts Shape: (batch_size, num_context_words)\n",
    "            batch_contexts.append(contexts)\n",
    "            # Batch Negative Samples Shape: (batch_size, num_negative_samples)\n",
    "            batch_neg_samples.append(negative_samples)\n",
    "            # Batch Weights Shape: (batch_size, num_context_words)\n",
    "            batch_weights.append(weights)\n",
    "\n",
    "            if len(batch_targets) == batch_size:\n",
    "                # Convert to array for it to work with jit.vmap\n",
    "                batch_targets = jnp.array(batch_targets)\n",
    "                batch_contexts = jnp.array(batch_contexts)\n",
    "                batch_neg_samples = jnp.array(batch_neg_samples)\n",
    "                batch_weights = jnp.array(batch_weights)\n",
    "                \n",
    "                # Get initial loss and gradient\n",
    "                initial_loss = batched_loss_fn_jit(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "                gradients = jax.grad(batched_loss_fn_jit, argnums=(0, 1))(W_in, W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "\n",
    "                # Backtracking line search\n",
    "                # Descent Direction for W_in and W_out\n",
    "                d_in, d_out = -gradients[0], -gradients[1]\n",
    "                lr = initial_lr\n",
    "\n",
    "                while True:\n",
    "                    new_W_in = W_in + lr * d_in\n",
    "                    new_W_out = W_out + lr * d_out\n",
    "                    new_loss = batched_loss_fn_jit(new_W_in, new_W_out, batch_targets, batch_contexts, batch_neg_samples, batch_weights)\n",
    "                    \n",
    "                    # Check if the new loss meets the sufficient decrease condition\n",
    "                    # Exit loop if condition is met\n",
    "                    if new_loss <= initial_loss + eta * lr * (jnp.sum(gradients[0] * d_in) + jnp.sum(gradients[1] * d_out)):\n",
    "                        break\n",
    "\n",
    "                    # Reduce alpha if condition is not met\n",
    "                    lr *= beta\n",
    "\n",
    "                # Update embeddings\n",
    "                W_in += lr * d_in\n",
    "                W_out += lr * d_out\n",
    "\n",
    "                # Reset batches\n",
    "                batch_targets, batch_contexts, batch_neg_samples, batch_weights = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: One of the words 'apple' or 'fruit' is not in the vocabulary.\n",
      "Cosine similarity between 'king' and 'queen': 0.8268\n",
      "Cosine similarity between 'man' and 'woman': 0.9763\n",
      "Cosine similarity between 'king' and 'man': 0.8962\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# Function to compute cosine similarity using JAX\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = jnp.dot(vec1, vec2)\n",
    "    norm_a = jnp.linalg.norm(vec1)\n",
    "    norm_b = jnp.linalg.norm(vec2)\n",
    "    if norm_a == 0 or norm_b == 0:  # Prevent division by zero\n",
    "        return 0.0\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Example word pairs and their expected similarities\n",
    "word_pairs = [\n",
    "    ('king', 'queen'),  # Expect high similarity\n",
    "    ('man', 'woman'),   # Expect high similarity\n",
    "    ('king', 'man'),    # Moderate similarity\n",
    "    ('apple', 'fruit'),  # Depending on your vocabulary, could expect a similarity\n",
    "]\n",
    "\n",
    "# Function to evaluate similarities using W_out\n",
    "def evaluate_similarity(vocab_idx, W_out, word_pairs):\n",
    "    scores = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in vocab_idx and word2 in vocab_idx:\n",
    "            vec1 = W_out[vocab_idx[word1]]  # Get the output embedding for word1\n",
    "            vec2 = W_out[vocab_idx[word2]]  # Get the output embedding for word2\n",
    "            cos_sim = cosine_similarity(vec1, vec2)\n",
    "            scores.append((word1, word2, cos_sim))\n",
    "        else:\n",
    "            print(f\"Warning: One of the words '{word1}' or '{word2}' is not in the vocabulary.\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Run the evaluation\n",
    "similarity_scores = evaluate_similarity(vocab_idx, W_out, word_pairs)\n",
    "\n",
    "# Print results\n",
    "for word1, word2, sim in similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Skip-Gram with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Softmax is an alternative to Negative Sampling, where the softmax function is approximated using a binary tree. The loss function is structured similarly to SGNS but involves traversing the binary tree structure and computing a conditional probability at each node.\n",
    "\n",
    "In this case, the loss function minimizes the difference between the predicted probability and the actual label for each word pair, traversing the tree for each prediction. The key idea is to reduce the computational complexity of softmax when dealing with a large vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CBOW, the objective is to predict the target word ​$w_t$ given the context words $w_c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  CBOW with Negative Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes the average of the context word embeddings and tries to predict the target word. The loss function for CBOW with Negative Sampling is also binary cross-entropy but formulated slightly differently due to the averaging of context word embeddings.\n",
    "\n",
    "The loss function is\n",
    "$$ L = - \\left[ \\log \\sigma \\left( \\langle v_{w_t}, v_C \\rangle \\right) \n",
    "             +  \\sum_{w_n \\in \\text{negatives}} \\log \\sigma \\left( \\langle -v_{w_n}, v_C \\rangle \\right) \\right] $$  \n",
    "where \n",
    "$$ v_C = \\frac{1}{|C|} \\sum_{w_c \\in C} v_{w_c} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW with Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Skip-Gram, CBOW can also use hierarchical softmax as an alternative to Negative Sampling, traversing a binary tree to approximate the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.sents(categories=\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all words, remove all punctuations and stop words\n",
    "def preprocess_sentence(sentence):\n",
    "    return [word.lower() for word in sentence if word.lower() not in stopwords and word not in punctuation]\n",
    "\n",
    "corpus_preprocessed = [preprocess_sentence(sentence) for sentence in corpus]\n",
    "corpus_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Window size for Co-occurrence matrix - 2 words before, 2 words after\n",
    "window_size = 2\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "def build_cooccurrence_matrix(corpus_preprocessed, vocab_idx, window_size):\n",
    "    cooccurrence_matrix = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for sentence in corpus_preprocessed:\n",
    "        sentence_length = len(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            word_idx = vocab_idx[word]\n",
    "            # Context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(sentence_length, i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                # Skip target word\n",
    "                if i != j:\n",
    "                    context_word = sentence[j]\n",
    "                    context_word_idx = vocab_idx[context_word]\n",
    "                    # Increment the co-occurrence count with inverse distance weighting\n",
    "                    cooccurrence_matrix[word_idx][context_word_idx] += 1.0 / abs(i - j)\n",
    "    return cooccurrence_matrix\n",
    "\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(corpus_preprocessed, vocab_idx, window_size)\n",
    "cooccurrence_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "embedding_dim = 50\n",
    "\n",
    "# Initialize word vectors and bias\n",
    "key = jax.random.PRNGKey(0)\n",
    "v = jax.random.normal(key, (vocab_size, embedding_dim))\n",
    "key, subkey = jax.random.split(key)\n",
    "v_tilde = jax.random.normal(subkey, (vocab_size, embedding_dim))\n",
    "bias = jnp.zeros(vocab_size)\n",
    "bias_tilde = jnp.zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight function\n",
    "def f(x):\n",
    "    return jax.minimum(1.0, (x/x_max)**alpha)\n",
    "\n",
    "# loss function\n",
    "def gLove_loss(v, v_tilde, bias, bias_tilde, cooccurrence_matrix):\n",
    "    loss = 0.0\n",
    "    for word in cooccurrence_matrix:\n",
    "        for context_word in cooccurrence_matrix[word]:\n",
    "            X_ij = cooccurrence_matrix[word][context_word]\n",
    "            weight = f(X_ij)\n",
    "            diff = jnp.dot(v[word], v_tilde[context_word]) + bias[word] + bias_tilde[context_word] - jnp.log(X_ij)\n",
    "            loss += weight * diff**2\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
